// ------------------------------------------------------
// Cross References
// ------------------------------------------------------
<-- /Users/thomaschang/Documents/dev/git/gratia/spark
	* Show cases
		- spark project
		- spark algo + unit test (ProfileJob + Spec)
		- how to run spark-submit from local spark terminal (Profile.scala)
		- how to use profile utility on sparkShell (ProfileUtil)

<-- Scratchapd
	* Snippet - Spark

<-- sh
	* bash_salesforce.sh
	* bash_datapipeline.sh

<-- Notebooks with ammonite
	* /Users/thomaschang/Documents/dev/git/tutorial/amm_almond/
		- SubjectLinePatterns
		- sparkNLPBook
		- to start
			jupyter notebook
	* Scratchpad
		- /Users/thomaschang/Documents/dev/scratchPad/tools/Snippets
		
	
// ------------------------------------------------------
// Operation
// ------------------------------------------------------
- Start SparkShell
	sparkShellWithPackageAvro
	sparkShellWithJar

- Sample codes
	import org.apache.spark.sql.SparkSession
	val session: SparkSession = SparkSession.builder().getOrCreate()
	val sparkContext = session.sparkContext
	val sqlContext = session.sqlContext
	val conf = new SparkConf()

	val df = Seq("I am a shiny Dataset!").toDF
	val ds = Seq("I am a shiny Dataset!").toDS


- val session: SparkSession = SparkSession.builder().getOrCreate()
	For spark 2+, use session to access entrypoints: sql|sparck context. HiveContext is depercated bc sparkSession supports hive support

	* val sparkContext = session.sparkContext

	* val sqlContext = session.sqlContext
		for sql like operations over datasets and dataframes

	

- show import history, identifying sources of names
	:imports

	when one types spark-shell, it imports a sequence of imports, which includes import spark.implicits._


- How to Parse and read a schema
	* Create mock data
		import org.apache.spark.sql.types.{IntegerType, StringType, ArrayType, StructType}
		import org.apache.spark.sql._

		val arrayStructData = Seq(
		  Row("James",List(Row("Java","XX",120),Row("Scala","XA",300))),
		  Row("Michael",List(Row("Apple","XY",200),Row("Tesla","XB",500))),
		  Row("Robert",List(Row("Dress","XZ",400),Row("PUrse","XC",250)))
		  ,Row("Washington",null)
		)

		// MAIN IDEA
		// - Mapping from  schema to object instantiation
		//        Each DF/RDD row --> Row
		//        schema.ArrayType  --> WrappedArray
		//.       schema.StructType -> Case class or Tuple
		// - One can create an arry of Struct with a group -> collect_set; see https://sparkbyexamples.com/spark/spark-dataframe-array-of-struct/
		val arrayStructSchema = new StructType() // ROW 
		    .add("name",StringType)
		    .add("booksIntersted",               
		         ArrayType(new StructType()      // LIST[ ROW ]
		                   .add("name",StringType)
		                   .add("author",StringType)
		                   .add("pages",IntegerType))
		        )

		val dfTmp1 = spark.createDataFrame(spark.sparkContext.parallelize(arrayStructData),arrayStructSchema)

	* Schema
		dfTmp1.printSchema
		 |-- name: string (nullable = true)
		 |-- booksIntersted: array (nullable = true)
		 |    |-- element: struct (containsNull = true)
		 |    |    |-- name: string (nullable = true)
		 |    |    |-- author: string (nullable = true)
		 |    |    |-- pages: integer (nullable = true)

	* Read the 
		// Code.Seq --> Schema.Array
		// Code.Row --> Schema.Struct
		import scala.util.{Try, Success, Failure}
		import org.apache.spark.sql.functions._
		import spark.implicits._ // THIS Import primitive encoders, which converts between JVM object and runtime representations
		
		dfTmp1
		    .filter(col("booksIntersted").isNotNull)
		    .select("booksIntersted")
		    .rdd
		    .map{ r : Row => // Iterate through each row of my DataFrame
		            // schema.booksIntersted is Array[Struct], --> code.return_type.Seq[Row]
		            val seqs: Seq[Row] = r.getAs[Seq[Row]]("booksIntersted")

		            seqs.map{ s: Row => // s is Row because schema.element is struct
		                // Return type of name is String
		                
		                // Try(null) returns None
		                val nameRow: String = Try{s.getAs[String]("name")}.getOrElse("")
		                nameRow
		            }
		    }.collect

		Outputs
			Array(
			  ArrayBuffer("Java", "Scala"),
			  ArrayBuffer("Apple", "Tesla"),
			  ArrayBuffer("Dress", "PUrse")
			)

- How to Read from RDD[String]
	val subjectLines: RDD[String] = rddSequence.map{r => r.flatten.mkString(" ")}

	// NOTICE that you don't need to use case matching.
	subjectLines.map{s1: String => methodA(sl) }


- One can also test via spark-submit
	* write test code in the class's companion object
		Ex: krux-kortex.git/DynamicPrefixSpanDistributed.scala

		sbt assembly
		sbt 'project distributed' compile assembly 

		spark-submit --class  --class com.krux.kortex.distributed.fp.PrefixSpanDistributed --deploy-mode client --master local\[4\] --driver-memory 1g --executor-memory 1g --jars [INCLUDE JAR YOU MAY NEED YOUR PROGRAM] /Users/thomaschang/Documents/dev/git/salesForce/krux-kortex/distributed/target/scala-2.12/krux-kortex-distributed-assembly-1.26.3.jar

- Example on how to add companion object to a class
	git: krux/kortex

	- CODE
		object DynamicPrefixSpanDistributed {
			 import org.apache.spark.SparkContext
			 import org.apache.spark.SparkConf
			 import org.apache.spark.rdd.RDD
			
			 def time[R](block: => R): (R, Double) = {
			   val t0 = System.nanoTime()
			   val result = block    // call-by-namea
			   val t1 = System.nanoTime()
			   val benchmarkSec = (t1 - t0)/ 1000000000.toDouble
			   (result, benchmarkSec)
			 }
			
			 val supportNum =  3
			 val numMocks = 4
			
			 def createRDD(numTenMultiples: Int): RDD[String] = {
			   val conf = new SparkConf().setAppName("Test").setMaster("local")
			   val sparkContext = new SparkContext(conf)
			   sparkContext
			     .parallelize(
			       (0 until numTenMultiples).foldLeft(scala.collection.mutable.ListBuffer[String]()) {
			         (accum, currIteration) => accum +Seq(
			           "When You Obtain This Jacket The World Is Your Oyster!",
			           "You Know That You Need This New Coat!",
			           "This Coat Will Make You Stand Out From The Pack!",
			           "Do Not Pass Go, Do Not Collect $200, Wear This Coat!",
			           "What Sort of Coat Do You Dream Of At Night?",
			           "When You Need A Coat You Purchase This Coat!",
			           "Here's a 33% Coupon For a Great Jacket!",
			           "The Best Things in Life Are This Coat",
			           "The Best Coat is The Coat That You Want",
			           "Buy This Really Nice Coat!"
			         )
			       }
			     )
			 }
			
			val rddPreprocessed = createRDD(numMocks)
			
			val database = rddPreprocessed.flatMap {s => if (s==null) None else Some(s.split(" ").toList)}
			
			val (prefixSpans, timeKruxGenPrefixSpan) = time {
			   val model = new PrefixSpanDistributed[String](database, supportNum)
			   model.mineForPatterns()
			
			}
			println(s"timeKruxGenPrefixSpan=$timeKruxGenPrefixSpan")
		}

	- sbt 'project distributed' compile package

	- spark-submit --class com...PrefixSpanDistributed --deploy-mode client --master local\[4\] --driver-memeory 1g --executor-memory 1g /Users/.../krux-kortext-distributed-assembly*.jar


	- Unit tests for spark
		val conf = new SparkConf().setAppName("Test").setMaster("local")
	    val sparkContext = new SparkContext(conf)

	    "should return same prefixSpans as SparkMLLIB" in {
		      val supportNum = 3L

		      val database = createRDD().map {_.split(" ").toList}
		      val modelKrux = new PrefixSpanDistributed[String](database, supportNum)
		      val prefixSpansKrux = modelKrux.mineForPatterns().keySet.map(_.mkString(" "))

		      val modelMLLIB = new org.apache.spark.mllib.fpm.PrefixSpan()
		        .setMinSupport(supportNum.toDouble / database.count)
		        .run(createRDD().map(s => s.split(" ").map(t => Array(t))))

		 }


// ------------------------------------------------------
// Concepts
// ------------------------------------------------------
<-- RDD vs Dataframe
	* sparkSql (Dataframe) is POTENTIALLY faster because of the 
		- query optimzer (Catalyst)
			* ie move filter before selections
			* collect status about dataset for cost-based optimization (ie reorder joins )
			* compile more efficient physical plan, thereby reduce garbage collection
		- fast in memory encoding for tabular data

	* RDD has these beneffits
		- is more expressive
			someone with scala knowledge can ramp up without learning new APIs
			scala code is more expressive than what is supported with Dataframe functions

		- Compile time type safety
			* Dataframe is untyped, ie Dataset[Row]

		- Unlike Dataframe, we can have more datatypes

	* https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset

<-- Encoders
	* Responsible for between JVM objects and runtime representation.

	* SparkSql comes with some default classes, and can be brought in with spark.implicits._

	* Encoders, aka serialization, are incurredd on shuffle operations. Otherwise, it is not an issue

<-- Spark Row
	 * Row in Spark is an ordered collection of fields that can be accessed starting at index 0. The row is a generic object of type Row.  IMOW: A row behavors like a tuple

	 * Row is NOT a tuple
	 	- DataFrame = Dataset[Row]
	 		df.map{ r: Row: 
	 			val var1 = r.getAs[String](colName)
	 		}

	 	- RDD can work with case classes and not just rows
	 		object EngagementEventDS {
	 			case class R(
				    subscriberId: Int, // entity
				    mid: Long,
				    jobId: Long,
				    batchId: Long,
				    event: Int, // a value of 1 ~ 4 with below xxxEvent values
				    confidence: Double,
				    utcTime: Timestamp
				 )
			}

			 val result: RDD[EngagementEventDS.R] =  df
			 	.rdd
			 	.map{ r => EngagementEventDS.R( 
					r.subscriberId,
					r.mid,
					r.jobId,
					r.batchId,
					EngagementEventDS.UnsubEvent,
					1.0,
					Timestamp.valueOf(r.utcTime)
				}			 

	 	- RDD can operate on tuples as well, giving us the ability to just use r._1
	 		Example shows how to work with DF and then RDD

				 val dfEntropy = dfRaw
					.rdd         // map engagementRatio to classes
				    .map{ r: Row =>
				        val engagementRatio = r.getAs[Double]("engagementRatio")
				        val label = globalEngagementRatioQuantile.search(engagementRatio).toString.replaceAll("InsertionPoint", "class")        
				        (r.getAs[Long]("clusterId"), label.toString )
				    }
				    .aggregateByKey(Seq.empty[String]) (
				        (seq: Seq[String], s: String) => s +: seq, 
				        (seqA: Seq[String], seqB: Seq[String]) => seqA +seqB
				    )
				    .map{ case(clusterId, labels) =>  // NOTICE: since we return a tuple in the map method above, our patten match is a tuple and NOT a case Row               
				        val labelToCntMap = labels.groupBy(identity).mapValues(vs => vs.size)
				        val N = labelToCntMap.values.sum
				        val entropy = labelToCntMap.map{ case (k, n) => - (n.toDouble/N)*(math.log(n.toDouble/N)) }.sum        
				        (clusterId, entropy)
				    }

				   val entropy: Array[Double] = dfEntropy.collect.map(_._2) // We can use _._2 because it is a RDD of tuples

	* Different ways to read value from rdd
		Approach 1: rdd.map{ r: Row =>
				val x = r.getAs[String]("commonSubsequence")
			}

		Approach 2: rdd.map { case Row(x:String, y:Int, z) ==> }

		Approach 2 is programatically more elegant, but is not robust to handle run time exceptions, lke nulls


// ------------------------------------------------------
// Architecture
// ------------------------------------------------------
- Spark Catalog
	* Interface to interact with the metastore
	* metastore contains metadata of tables; 
		- when one reads a table, you need to know the type of the column
		

// ------------------------------------------------------
// File Formats
//	 	AVRO vs Parquet vs ORC
// ------------------------------------------------------
- Reference: 
	https://www.clairvoyant.ai/blog/big-data-file-formats#:~:text=AVRO%20vs.&text=AVRO%20is%20a%20row%2Dbased,are%20better%20than%20in%20PARQUET.

- AVRO
	* Data is stored in rows format 
		- good for writing data
		- good when you need all the columns

- Parquet
	* Data is stored in column format
		- good when the computation load is heavy on 1 column analytics
	* supports storing nested structure
	* schema evolution: only column append
		avro is more advance

- ORC: Optimized Row Columnar	
	* optmized for efficient storage


- Comparisons
	* AVRO vs. PARQUET
		- AVRO is a row-based storage format, whereas PARQUET is a columnar-based storage format.
		PARQUET is much better for analytical querying, i.e., reads and querying are much more efficient than writing.
		
		- Writiing operations in AVRO are better than in PARQUET.
		
		- AVRO is much matured than PARQUET when it comes to schema evolution. PARQUET only supports schema append, whereas AVRO supports a much-featured schema evolution, i.e., adding or modifying columns.
		
		- PARQUET is ideal for querying a subset of columns in a multi-column table. AVRO is ideal in the case of ETL operations, where we need to query all the columns.
	
	* ORC vs. PARQUET
		- PARQUET is more capable of storing nested data.
		
		- ORC is more capable of Predicate Pushdown.
		
		- ORC supports ACID properties.
		
		- ORC is more compression efficient.



// ------------------------------------------------------
// Spark Efficiency
// ------------------------------------------------------
<-- Serialization is only an issue when data is shuffled
	* Shuffle is created by groupBy, aggregateByKey, join,
		Shuffle will also cause driver memory loads

	* Ex
		Given: val rdd = sc.parallelize(1 to 4, 4).map(i => (i, new Foo(i)))

		No serialization: rdd.count

		Serialization cost: rdd.distinct.count


<-- ReduceByKey|AggregateByKey vs GroupByKey vs Groupby
	* References
		https://harshitjain.home.blog/2019/09/08/groupbykey-vs-reducebykey-vs-aggregatebykey-in-apache-spark-scala/

		https://stackoverflow.com/questions/24804619/how-does-spark-aggregate-function-aggregatebykey-work

		http://balajireddyblog.blogspot.com/2016/12/groupby-vs-groupbykey-in-this-posti.html

	* Faster:  ReduceByKey|AggregateByKey < GroupByKey < GroupBy : Slower,Flexible
		
		RDD:
			groupBykey: group data which causes data shuffling when RDD is not already partitioned

			reduceByKey: group key with aggregation; shuffle less data than group by key
				More efficient because Spark knows it can combine output with a common key on each partition before shuffling the data

			aggregateByKey: logically same as reduceByKey, but lets you return a differnt type


		Example1:
			val pairs_for_test = sc.parallelize(Array(("a", 2), ("a", 3), ("a", 4), ("b", 7), ("b", 5), ("a", 10), ("b", 15)));

			groupbyKey
				val res_groupByKey=pairs_for_test.groupByKey().map(x=> (x._1, x._2.sum));
				Array[(String, Int)] = Array((a,19), (b,27)) 

			reduceByKey
				val res_reduceByKey = pairs_for_test.reduceByKey(_ + _);
				 Array((a,19), (b,27))

			aggregateByKey(initValue)(combineFunc, reduceMergeFunc)
				val res_aggregateByKey = pairs_for_test.aggregateByKey(0)(_+_, _+_);
				 Array((a,19), (b,27))

				mapperFnc -> combinerFnc --> DATA SHUFFLE --> ReducerFunc
				
				combinerFunc: fnc that this applied before data is shuffled to the other nodes
					This function accepts two parameters. The second parameter(which is the value being gruped) is merged into the first parameter(which is the accumulator?). This function combines/merges values within a single partition.

				reduceFunc: 

				Example
					df
					.rdd //2 : map engagementRatio to classes
				    .map{ r: Row =>
				        val engagementRatio = r.getAs[Double]("engagementRatio")
				        val label = globalEngagementRatioQuantile.search(engagementRatio).toString.replaceAll("InsertionPoint", "class")    
				        (r.getAs[Long]("clusterId"), label.toString )
				    }
				    .aggregateByKey(Seq.empty[String]) (
				        (seq: Seq[String], s: String) => s +: seq, //combiner
				        (seqA: Seq[String], seqB: Seq[String]) => seqA +seqB //reducer
				    )

		Takeaway
			- avoid groupByKey when performing an associative reductive operation, instead use reduceByKey
				For example, rdd.groupByKey().mapValues(_.sum) will produce the same results as rdd.reduceByKey(_ _). However, the former will transfer the entire dataset across the network, while the latter will compute local sums for each key in each partition and combine those local sums into larger sums after shuffling.

			- Avoid reduceByKey When the input and output value types are different instead use aggregateByKey.



		ReduceByKey Code
			reduceByKeys works with single value keys; if you want to work with keys that are tupled, need to extend RDD. Look at optimiedReduceByKey which comes from ExtendedRDDFunctions, defined in Krux

			object Job extends ExtendedRDDFunctions {
			    val sales: RDD[(String, String, Double, Int)]= sparkSession.sparkContext.parallelize(List(
			       ("West",  "Apple",  2.0, 10),
			       ("West",  "Apple",  3.0, 15),
			       ("West",  "Orange", 5.0, 15),
			       ("South", "Orange", 3.0, 9),
			       ("South", "Orange", 6.0, 18),
			       ("East",  "Milk",   5.0, 5)))

			    // Works By Default
			    def runSimpleKV() : RDD[(String, Double)] = {
			        val a  = sales.map{ case(store, prod, amt, units) => ( store, amt ) } 
			        a.reduceByKey( (x, y) => (x y)) //Out of the box reduceByKey only works with single key value, ie store
			    }

			    def runSimpleTuple() : RDD[( (String, String), Double)] = {
			        val a  = sales.map{ case(store, prod, amt, units) => ( (store, prod), amt ) } 
			        a.optimizedReduceByKey( (x, y) => (x y))       
			    }
			    
			    def runPrefixSpanV1() /*: RDD[( Long, String, Int, Int )]*/ = {
			        val a = rddClusterInferred.map{ case(eid, subject, delivered, engages) => ( (eid, subject), (delivered, engages) /*delivered, engages*/) }
			        a
			            .optimizedReduceByKey( (x, y) => (x._1 y._1, x._2 y._2) )
			            .map( t => (t._1._1, t._1._1, t._2._1, t._2._2) )
			    }
			}


<-- MapPartition vs Map
	mapPartition applies a function per PARTITION; map applies the function per RECORD

	If there is a initialization step, ie create db connector or derive some partition statistics, use mapPartition. Map partition needs to return an iterator

		- Ex: partition statistics
			def findFrequentOnePattern(iter: RDD[List[A]]): Map[A, Long] = {
				iter
		        .map(_.toSet)
		        .mapPartitions { rows =>
		          Iterator(
		            rows.toSeq.flatten.groupBy(identity).map { case (k, v) => (k, v.size.toLong) }
		          )
		        }
		        .reduce { case (statMapA, statMapB) =>
		          statMapA.foldLeft(statMapB) { case (res, (k, scA)) =>
		            res (k -> res.get(k).map(_ scA).getOrElse(scA))
		          }
		        }

		- Ex: DB connector
			Reference: https://stackoverflow.com/questions/21185092/apache-spark-map-vs-mappartitions

			val newRd = myRdd.mapPartitions(partition => {
				  val connection = new DbConnection /*creates a db connection per partition*/

				  val newPartition = partition.map(record => {
				    readMatchingFromDB(record, connection)
				  }).toList // consumes the iterator, thus calls readMatchingFromDB 

				  connection.close() // close dbconnection here
				  newPartition.iterator // create a new iterator
				})

- Logical vs Physical paln
	
	case class Item(id: Int, name: String, price: Double)
	case class Order(id: Int, itemId: Int, count: Int)

	val itemsDF = Seq(
	    Item(0, "Tomatoe", 2.0),
	    Item(1, "Watermelon", 5.5),
	    Item(2, "Pineapple", 7.0),
	    Item(3, "Banana", 1.5)
	).toDF()

	val ordersDF = Seq(
	    Order(100, 0, 1),
	    Order(100, 1, 1),
	    Order(101, 2, 3),
	    Order(102, 2, 8)
	).toDF()


	val df1 = itemsDF.join(ordersDF, itemsDF("id") === ordersDF("itemId"), "inner")

	df1.explain() // print logical + physical plans

	df1.queryExpection.logical // only logical

	import org.apache.spark.sql.catalyst.plans.logical._
	val sortedOrder = x.queryExecution.logical.output.filter(attr => RowOrdering.isOrderable(attr.dataType)).map(SortOrder(_, Ascending))


// ------------------------------------------------------
// Spark Encoders
// ------------------------------------------------------
- References
	https://spark.apache.org/docs/2.3.0/sql-programming-guide.html

	
- RDD uses Java or Kryo serialization
	Java serialization, which is the default, is slow and result in big memory
	Kryo is faster than, but rquires registering; a bit more work

- DF and dataset uses Encoders and Decoders
	* Encoders converts objects to spark schema (struct STructType)
		Spark comes with some default encodrs like Int, and STring

		encoders are code generated dynamically(StructType) and use a format that allows Spark

	* Encoders are IMPLICLITY created for case classes, which implements the Product trait !!
		
		Spark Dataset Encoders has compile check for fields that extends the Product type, ie case class. But if the field does not, ie java.uilt.Date, it will not catch the error (ie Donglin's commit)

	* Ex: 
		case class Person(name: String, age: Long)
		val caseClassDS = Seq(Person("Andy", 32)).toDS()
		val caseClassDF = Seq(Person("Andy", 32)).toDF()




// ------------------------------------------------------
// Spark Gotchas
// ------------------------------------------------------
- The function serialized to the executor cannot refer a dataframe
	* 

// ------------------------------------------------------
// Spark Performances
// ------------------------------------------------------
<-- Common cause of performance problems
	Spill to disk 
		out of memory; to much cache
	
	Shuffle
		Reduce shuffling oerators
		
		if use rdd, use combiner to reduce amount of data transfered

		use efficient serialization; java serializer is inefficient


	Skew
		Data partition should have similar size
		use salt joins,


<-- Tune the number of partitions
	- each partition 10-50G
	- numPartition = 2 * numCores
	- code
		* all code below is part of utility
		(1) Find number of bytes on director
			val (fileCnt, fileInByteSize) = util.computeFileStats(dir)
		(2) Find number of Spark records
			val numSparkRecords = spark.read.parquet(dir).count
		(3) Desired partition size
			val targetPartitionSizeGB = 25 // usually between 10-50
		(4) calculate num partition
			val bytesPerRecord = fileInBytesSize / numSparkRecords
			val numRecordPerPartition = targetParititonSizeGB / bytesPerRecord

			val numParition = Math.round(numSparkRecrds / numRecordPerPartition)




val df = Seq("I am a shiny Dataset!").toDF
df.count

 


// ------------------------------------------------------
// Frameless (TypeLevel)
// ------------------------------------------------------
<-- TypedDataset
	* reference: http://typelevel.org/frameless/TypedDatasetVsSparkDataset.html
	* TypedDatasets allows for 
		(B1) type-safe syntax : can catch type error at compile time, so we don;t have to run entire spark jobs to find error.

		(B2) with a fully optimized query plan.
			ex: spark catalyst (query optimizer) can load only the parquet column they need


		Ex1: Using spark dataset
			Setup
				case class Foo(i: Long, j: String)

				val in1itialDs = spark.createDataset( Foo(1, "Q") :: Foo(10, "W") :: Nil )

				initialDs.write.parquet("/tmp/foo")

				val ds = spark.read.parquet("/tmp/foo").as[Foo]

			With
				val filteredDs = ds.filter($"i" === 10).select($"i".as[Long])

				filterDS.explain() --> spark only loads the i column of the parqet data (ie B2)

				but we need to cast $"i".as[Long] to ensure type safety. If we cast wrong, we will only find out at job run time.

		Ex2: Using frameless typedDataset
			Setup
				import frameless.TypedDataset
				import frameless.syntax._
				val fds = TypedDataset.create(ds)
				
				fds.filter(fds('i) === 10).select(fds('i)).show().run() 

			With
				fds.filter(fds('i) === 10).select(fds('i)).explain()
					we have B2 because we load only column i.

				fds.filter(fds('i) === 10).select(fds('x))
					Furthermore, if select a nonexistent column x, we will catch at compile time.


		Ex3: Improving Encoder Type Check at Compile Time
			- Case class extends the scala Product trait
				The set of all possible values of a product type is its Cartesian product.
				ex: case class Student(enrolled: Boolean, age: Byte)
					// boolean=2 * byte=2^8=256 = 512
			- Scala product is used to represents an alegebraic data type

			- Spark Dataset Encoders has compile check for fields that extends the Product type, ie case class. But if the field does not, ie java.uilt.Date), it will ot catch the error (ie Donglin's commit)

	* Opinion:
		The biggest advantage i see is the typeclases to allow for non primative types in the datasets. Spark forces you to use primatives, and it's caused a big bloat in our codebase to support converting between custom types and primatives.

		ex: Krux codebase com.krux.sparkling.datasource.sql.util DatasetTypeConverter converts Option back to primitive types.

		ex: spark allows one to create complex data types via the DataType abstract class
			ref: https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-DataType.html

			val arrayType = DataTypes.createArrayType(BooleanType)



// ------------------------------------------------------
// Prod Support n AWS
// ------------------------------------------------------
<-- How to Tell Skew?
	* Get the EMR Cluster that ran the job
		- List Pipelines 
		- scroll till first error 
		- Click Deppendencies Tab
		- Read the TaskRunner Logs
			To find the EMR cluster, search for "TaskRunnerService"
			ex: 
				TaskRunnerService-df-0443544169WW4J69MP5T_@EmrCluster_6e74e793-e29b-40fa-bf5d-8d0ff6a31a45_2022-10-17T10:00:00-2
		- Go to EMR Cluster page
			refresh the page
			search for df-0443544169WW4J69MP5T
				EMR Cluster= takes you to EMR Cluster  that ran the task

				https://us-east-1.console.aws.amazon.com/elasticmapreduce/home?region=us-east-1#cluster-details:j-33DW1T169XRCL

		- From EMR Cluster for this activity, one has access to:
			(1) Table: High-level application history
				
				Each application consist of mulitple jobs
				
				load all applications
				
				If sort task by duration, one sees 2 applications tok a very long time. Click on application, ie application_1666001699669_0050

				We will see 3 tabs: Jobs, Stages, Executors.  All 3 tabs will show success, failures from a job, stage, or executor perspective. Since skew is a join problem, let's look at stages. 
				
				stages tab:
					Breaks success and failure by stage
					
					click on the failed stage (12), which showed the error failed at flatMap at StoAnalyzer.scala line 48
						breaks by code and shows duration, #shuffle,

					see by 39 million records being shuffled!



				job tab:
					Breaks success/failure by job


			(2) Link: Spark History Server
				Same info can be accessed here; 

				Has Dag Visualization
				

	* Pointers
		- Find the previous step to the failure step, the failure is in between
		typical canidiates: shuffling, groupByKey, join, reduce depending on reducing function

		- step1 out of 100; skewness appeared for 1 task that keep failing.  Datanode: this taskwill try to move to another executor

		- early tasks fail early on, then it is very likely not skew
